{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Hazırlama (Data Preprocessing)\n",
    "Bu bölümde dataset nasıl bir değişikliklere uğradı veri nerelerden toplandı gibi konulara değinilecek. Yani kısacası dataset uğradığı bütün değişiklikler anlatılacaktır. Burada değinilen aşamalar gerçekleştirilmesi sonrasında yeni bir dataset elde edildi. Preprocess işlemleri sonrası detaylar *`main.ipynb`* dosyasında ayrıntılı bir şekilde anlatılmıştır. Dataset'in toplanmasında 2 kaynak kullanılmıştır (Kaggle: Youtube Trending Video Dataset, Youtube API).\n",
    "**NOT:** Bu dosyada bulunan kodlar tek seferlik çalıştırma gereği olduğu için daha önceden çalıştırılıp dataset üzerinde düzenlemeler yapılmıştır. Bu dosyada sadece yapılan işlemler değinilmiştir."
   ],
   "id": "6b7193954849ec1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Kullanılan Kütüphaneler",
   "id": "8f870c432f6ccbe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "234b4217102bb951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "&nbsp;\n",
    "### Dataset'in Okunması"
   ],
   "id": "65a8d9371669f591"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "FILE_PATH = \"dataset.csv\"\n",
    "df = pd.read_csv(FILE_PATH)"
   ],
   "id": "331dc59e36d61217"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "&nbsp;\n",
    "### Sütunların temizlenmesi\n",
    "Birleştirilen dataset te bazı istenilmeyen sütunlar vardı (dislike ve ratings_disabled) bu veriler 2022 sonrasında artık Youtube API'dan kaldırılmıştır O yüzden bizim kaggle'dan alınan datasetten silinmesi gerekiyordu.\n"
   ],
   "id": "b769fe277bcd4d42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cols_to_delete = [\"ratings_disabled\", \"dislikes\"]\n",
    "for col in cols_to_delete:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "        print(f\"'{col}' sütunu silindi.\")\n",
    "    else:\n",
    "        print(f\"'{col}' sütunu zaten yok.\")"
   ],
   "id": "4612043a03b30789"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "&nbsp;\n",
    "### Sayısal değerlere dönüştürme\n",
    "True ve False olan bazı sütunlar sayısal değerlere 0,1 olarak dönüştürülmüştür (comments_disabled)."
   ],
   "id": "443882f813261c4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if \"comments_disabled\" in df.columns:\n",
    "    df[\"comments_disabled\"] = df[\"comments_disabled\"].replace({\n",
    "        True: 1,\n",
    "        False: 0,\n",
    "        \"True\": 1,\n",
    "        \"False\": 0,\n",
    "        \"true\": 1,\n",
    "        \"false\": 0\n",
    "    })\n",
    "\n",
    "    df.to_csv(FILE_PATH, index=False)\n",
    "    print(\"'comments_disabled' sütunundaki True/False değerleri 1 ve 0 olarak dönüştürüldü\")\n",
    "else:\n",
    "    print(\"'comments_disabled' sütunu bulunamadı.\")\n"
   ],
   "id": "4da87d908d37730f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "&nbsp;\n",
    "## Veri'nin Toplanması\n",
    "Kaggle'dan aldığımız dataset'te sadece trend olan videoların verisi vardı ve bizim modelimizin tahmin yapabilmesi için non trend video verilerine ihtiyacımız vardı. İşte burada Youtube Data API v3 devreye giriyor ve aşağıdaki kod yardımıyla bütün takım üyeleri veri çekmiştir bu API aracılığıyla. Tabi bunun çalışması için Google Cloud Console sitesinden API key oluşturulup veri çekimi için kod kısmına eklenmesi gerekir."
   ],
   "id": "b1d63ded34d78b66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "API_KEYS = []\n",
    "\n",
    "OUTPUT_FILE = \"../Dataset/non_trending_videos.csv\"\n",
    "MAX_RESULTS = 50\n",
    "TARGET_COUNT = 100000\n",
    "SAVE_INTERVAL = 1000\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"music\", \"movie\", \"vlog\", \"review\", \"funny\", \"gaming\", \"education\", \"sports\",\n",
    "    \"tutorial\", \"travel\", \"science\", \"art\", \"food\", \"comedy\", \"documentary\",\n",
    "    \"tech\", \"dance\", \"live\", \"shorts\", \"how to\", \"challenge\", \"reaction\",\n",
    "    \"asmr\", \"interview\", \"test\", \"study\", \"ai\", \"robotics\", \"fashion\", \"nature\"\n",
    "]\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existing = pd.read_csv(OUTPUT_FILE)\n",
    "    existing_ids = set(df_existing[\"video_id\"].astype(str))\n",
    "    print(f\"Existing records loaded: {len(existing_ids)} entries\\n\")\n",
    "else:\n",
    "    existing_ids = set()\n",
    "    print(\"New CSV will be created\\n\")\n",
    "\n",
    "all_videos = []\n",
    "total_count = 0\n",
    "api_index = 0\n",
    "quota_exhausted = [False] * len(API_KEYS)\n",
    "page_token = None\n",
    "\n",
    "def get_key():\n",
    "    global api_index\n",
    "    return API_KEYS[api_index % len(API_KEYS)]\n",
    "\n",
    "\n",
    "def next_key():\n",
    "    global api_index\n",
    "    quota_exhausted[api_index] = True\n",
    "    usable = [i for i, used in enumerate(quota_exhausted) if not used]\n",
    "\n",
    "    if not usable:\n",
    "        print(\"All API keys have reached their quota. Stopping data collection...\")\n",
    "        save_progress()\n",
    "        exit(0)\n",
    "\n",
    "    api_index = usable[0]\n",
    "    print(f\"Switched to next API key → {get_key()}\")\n",
    "\n",
    "def save_progress():\n",
    "    global all_videos\n",
    "    if not all_videos:\n",
    "        return\n",
    "    print(f\"Saving {len(all_videos)} videos...\")\n",
    "    temp_df = pd.DataFrame(all_videos)\n",
    "    mode = \"a\" if os.path.exists(OUTPUT_FILE) else \"w\"\n",
    "    header = not os.path.exists(OUTPUT_FILE)\n",
    "    temp_df.to_csv(OUTPUT_FILE, mode=mode, index=False, header=header)\n",
    "    all_videos.clear()\n",
    "    print(f\"{OUTPUT_FILE} updated.\")\n",
    "\n",
    "def fetch_video_stats(video_ids):\n",
    "    stats = {}\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        batch = video_ids[i:i + 50]\n",
    "        while True:\n",
    "            url = (\n",
    "                f\"https://www.googleapis.com/youtube/v3/videos\"\n",
    "                f\"?part=statistics,snippet&id={','.join(batch)}&key={get_key()}\"\n",
    "            )\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=10)\n",
    "                data = resp.json()\n",
    "\n",
    "                if \"error\" in data:\n",
    "                    msg = data[\"error\"][\"message\"]\n",
    "                    print(f\"{msg}\")\n",
    "                    if \"quota\" in msg.lower():\n",
    "                        next_key()\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                for item in data.get(\"items\", []):\n",
    "                    vid = item[\"id\"]\n",
    "                    s = item.get(\"statistics\", {})\n",
    "                    snippet = item.get(\"snippet\", {})\n",
    "                    stats[vid] = {\n",
    "                        \"view_count\": int(s.get(\"viewCount\", 0)),\n",
    "                        \"likes\": int(s.get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(s.get(\"commentCount\", 0)),\n",
    "                        \"categoryId\": int(snippet.get(\"categoryId\", 0)),\n",
    "                        \"tags\": \", \".join(snippet.get(\"tags\", [])),\n",
    "                        \"comments_disabled\": 1 if \"commentCount\" not in s else 0,\n",
    "                        \"ratings_disabled\": 1 if \"likeCount\" not in s else 0\n",
    "                    }\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Error fetching video statistics:\", e)\n",
    "                next_key()\n",
    "                time.sleep(1)\n",
    "        time.sleep(0.15)\n",
    "    return stats\n",
    "\n",
    "def random_date_range_2022_2024():\n",
    "    start = datetime(2022, 1, 1)\n",
    "    end = datetime(2024, 12, 31)\n",
    "    delta = end - start\n",
    "    random_start = start + timedelta(days=random.randint(0, delta.days - 30))\n",
    "    random_end = random_start + timedelta(days=30)\n",
    "    return (\n",
    "        random_start.strftime(\"%Y-%m-%dT00:00:00Z\"),\n",
    "        random_end.strftime(\"%Y-%m-%dT23:59:59Z\"),\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Starting video collection (US region, 2022–2024 range)...\\n\")\n",
    "\n",
    "for _ in tqdm(range(TARGET_COUNT // MAX_RESULTS * 2), desc=\"API requests\"):\n",
    "    query = random.choice(KEYWORDS)\n",
    "    published_after, published_before = random_date_range_2022_2024()\n",
    "\n",
    "    search_url = (\n",
    "        f\"https://www.googleapis.com/youtube/v3/search\"\n",
    "        f\"?part=snippet&type=video&maxResults={MAX_RESULTS}\"\n",
    "        f\"&regionCode=US&q={query}&videoDuration=any\"\n",
    "        f\"&publishedAfter={published_after}&publishedBefore={published_before}\"\n",
    "        f\"&key={get_key()}\"\n",
    "    )\n",
    "\n",
    "    if page_token:\n",
    "        search_url += f\"&pageToken={page_token}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, timeout=10)\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(\"API request error:\", e)\n",
    "        next_key()\n",
    "        continue\n",
    "\n",
    "    if \"error\" in data:\n",
    "        msg = data[\"error\"][\"message\"]\n",
    "        print(f\"{msg}\")\n",
    "        if \"quota\" in msg.lower():\n",
    "            next_key()\n",
    "        continue\n",
    "\n",
    "    page_token = data.get(\"nextPageToken\", None)\n",
    "    items = data.get(\"items\", [])\n",
    "    new_ids = []\n",
    "\n",
    "    for item in items:\n",
    "        if item[\"id\"].get(\"kind\") != \"youtube#video\":\n",
    "            continue\n",
    "\n",
    "        vid_id = item[\"id\"].get(\"videoId\")\n",
    "        if not vid_id or vid_id in existing_ids:\n",
    "            continue\n",
    "\n",
    "        snippet = item[\"snippet\"]\n",
    "        all_videos.append({\n",
    "            \"video_id\": vid_id,\n",
    "            \"title\": snippet.get(\"title\", \"\"),\n",
    "            \"publishedAt\": snippet.get(\"publishedAt\", \"\"),\n",
    "            \"channelId\": snippet.get(\"channelId\", \"\"),\n",
    "            \"channelTitle\": snippet.get(\"channelTitle\", \"\"),\n",
    "            \"description\": snippet.get(\"description\", \"\"),\n",
    "            \"thumbnail_link\": snippet.get(\"thumbnails\", {}).get(\"default\", {}).get(\"url\", \"\"),\n",
    "            \"is_trending\": 0\n",
    "        })\n",
    "        existing_ids.add(vid_id)\n",
    "        new_ids.append(vid_id)\n",
    "        total_count += 1\n",
    "\n",
    "    if new_ids:\n",
    "        stats_data = fetch_video_stats(new_ids)\n",
    "        for v in all_videos[-len(new_ids):]:\n",
    "            vid = v[\"video_id\"]\n",
    "            if vid in stats_data:\n",
    "                v.update(stats_data[vid])\n",
    "\n",
    "    if total_count % SAVE_INTERVAL == 0 and total_count > 0:\n",
    "        save_progress()\n",
    "        print(f\"Total videos collected: {total_count}\\n\")\n",
    "\n",
    "    if total_count >= TARGET_COUNT:\n",
    "        print(\"Target reached!\")\n",
    "        break\n",
    "\n",
    "    time.sleep(0.25)\n",
    "\n",
    "save_progress()\n",
    "print(f\"\\nProcess completed. Total videos collected: {total_count}\")\n",
    "print(f\"CSV file: {os.path.abspath(OUTPUT_FILE)}\")\n"
   ],
   "id": "2800138c9a99b9e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Çekilen verilerin `publishedAt` feature'ında bulunan tarihler ile kaggle'dan alınan datasetin `publishedAt` feature'ının tarih formatıyla uyuşmuyordu. Bu yüzden O sütundaki tarihleri birbirleriyle denkleştirmek için aşağıdaki kod bloğundan yararlanılmıştır.",
   "id": "9c2922d0e74db8df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df[\"publishedAt\"] = pd.to_datetime(df[\"publishedAt\"], errors=\"coerce\", utc=True)\n",
    "df[\"publishedAt\"] = df[\"publishedAt\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "df.to_csv(FILE_PATH, index=False)\n",
    "\n",
    "print(\"Tarih formatları başarıyla ISO-Z formatına dönüştürüldü.\")\n"
   ],
   "id": "abec9c2247f61a15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bu işlem sonrasında orijinal(Kaggle'dan alınan) ve bizim elimizde olan datasetlere *`is_trending`* feature'ı ekleyip **Trend** verisi içeren kaggle dataset'i ile bizim çekmiş olduğumuz **Non Trend** verisi içeren datasetleri merge ederek şuan elimizde olan ve kullanmış olduğumuz US'te paylaşılan videoların *`Final_Dataset`*'ini elde etmiş oluyoruz.",
   "id": "cc952c2b47f259e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
